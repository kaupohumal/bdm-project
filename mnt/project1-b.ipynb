{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20d967e2-20dc-4cef-961a-bef47f2a9267",
   "metadata": {},
   "source": [
    "# Project 1 Report\n",
    "\n",
    "#### Joosep Orasm√§e, Kaupo Humal, Tanel Tiisler, Kalju Jake Nekvasil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7771bc3d-f45b-40d5-a88e-649837a30fae",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "This section is for importing libraries and visualizing the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25ef2470-8dcd-42e9-80b3-ac0861579b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "                    .appName('project1')\n",
    "                    .getOrCreate()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11c34c3d-9b70-4b59-a6ff-c9b3fdd925df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Window\n",
    "from shapely.geometry import Point, Polygon\n",
    "from shapely import from_wkt\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebe3c3ee-e6c4-470c-b05c-62d5f4a9068d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_df = (spark.read\n",
    "             .option(\"sep\", \",\") # separator\n",
    "             .option(\"header\", True) # file has header row\n",
    "             .option(\"inferSchema\", True) # spark tries to infer data types\n",
    "             .csv(\"input/Sample NYC Data.csv\") #path\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74d5daab-f9f3-468f-bdf2-9b629bd1ef03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- medallion: string (nullable = true)\n",
      " |-- hack_license: string (nullable = true)\n",
      " |-- vendor_id: string (nullable = true)\n",
      " |-- rate_code: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rides_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d87b57c1-ed4d-4ad8-bd0e-3e1302c13a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+---------+------------------+---------------+----------------+---------------+----------------+---------------+-----------------+----------------+\n",
      "|           medallion|        hack_license|vendor_id|rate_code|store_and_fwd_flag|pickup_datetime|dropoff_datetime|passenger_count|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|\n",
      "+--------------------+--------------------+---------+---------+------------------+---------------+----------------+---------------+----------------+---------------+-----------------+----------------+\n",
      "|89D227B655E5C82AE...|BA96DE419E711691B...|      CMT|        1|                 N| 01-01-13 15:11|  01-01-13 15:18|              4|      -73.978165|      40.757977|       -73.989838|       40.751171|\n",
      "|0BD7C8F5BA12B88E0...|9FD8F69F0804BDB55...|      CMT|        1|                 N| 06-01-13 00:18|  06-01-13 00:22|              1|      -74.006683|      40.731781|       -73.994499|        40.75066|\n",
      "|0BD7C8F5BA12B88E0...|9FD8F69F0804BDB55...|      CMT|        1|                 N| 05-01-13 18:49|  05-01-13 18:54|              1|      -74.004707|       40.73777|       -74.009834|       40.726002|\n",
      "|DFD2202EE08F7A8DC...|51EE87E3205C985EF...|      CMT|        1|                 N| 07-01-13 23:54|  07-01-13 23:58|              2|      -73.974602|      40.759945|       -73.984734|       40.759388|\n",
      "|DFD2202EE08F7A8DC...|51EE87E3205C985EF...|      CMT|        1|                 N| 07-01-13 23:25|  07-01-13 23:34|              1|       -73.97625|      40.748528|       -74.002586|       40.747868|\n",
      "|20D9ECB2CA0767CF7...|598CCE5B9C1918568...|      CMT|        1|                 N| 07-01-13 15:27|  07-01-13 15:38|              1|      -73.966743|      40.764252|       -73.983322|       40.743763|\n",
      "|496644932DF393260...|513189AD756FF14FE...|      CMT|        1|                 N| 08-01-13 11:01|  08-01-13 11:08|              1|      -73.995804|      40.743977|       -74.007416|       40.744343|\n",
      "|0B57B9633A2FECD3D...|CCD4367B417ED6634...|      CMT|        1|                 N| 07-01-13 12:39|  07-01-13 13:10|              3|      -73.989937|      40.756775|        -73.86525|        40.77063|\n",
      "|2C0E91FF20A856C89...|1DA2F6543A62B8ED9...|      CMT|        1|                 N| 07-01-13 18:15|  07-01-13 18:20|              1|      -73.980072|      40.743137|       -73.982712|       40.735336|\n",
      "|2D4B95E2FA7B2E851...|CD2F522EEE1FF5F5A...|      CMT|        1|                 N| 07-01-13 15:33|  07-01-13 15:49|              2|      -73.977936|      40.786983|       -73.952919|        40.80637|\n",
      "|E12F6AF991172EAC3...|06918214E951FA000...|      CMT|        1|                 N| 08-01-13 13:11|  08-01-13 13:19|              1|      -73.982452|      40.773167|       -73.964134|       40.773815|\n",
      "|E12F6AF991172EAC3...|06918214E951FA000...|      CMT|        1|                 N| 08-01-13 09:50|  08-01-13 10:02|              1|       -73.99556|      40.749294|       -73.988686|       40.759052|\n",
      "|78FFD9CD0CDA541F3...|E949C583ECF62C8F0...|      CMT|        1|                 N| 10-01-13 12:07|  10-01-13 12:17|              1|      -73.971497|      40.791321|       -73.964478|       40.775921|\n",
      "|237F49C3ECC11F502...|93C363DDF8ED9385D...|      CMT|        1|                 N| 07-01-13 07:35|  07-01-13 07:46|              1|       -73.98851|      40.774307|       -73.981094|       40.755325|\n",
      "|3349F919AA8AE5DC9...|7CE849FEF67514F08...|      CMT|        1|                 N| 10-01-13 15:42|  10-01-13 16:04|              1|      -73.994911|      40.723221|       -73.971558|       40.761612|\n",
      "|3349F919AA8AE5DC9...|7CE849FEF67514F08...|      CMT|        1|                 N| 10-01-13 14:27|  10-01-13 14:45|              1|      -74.010391|      40.708702|       -73.987846|       40.756104|\n",
      "|4C005EEBAA7BF26B8...|351BE7D984BE17DB2...|      CMT|        1|                 N| 07-01-13 22:09|  07-01-13 22:19|              1|      -73.973732|      40.756287|       -73.998413|       40.756832|\n",
      "|7D99C30FCE69B1A9D...|460C3F57DD9CB2265...|      CMT|        1|                 N| 07-01-13 17:18|  07-01-13 17:20|              1|      -73.968925|      40.767704|        -73.96199|       40.776566|\n",
      "|E6FBF80668FE0611A...|36773E80775F26CD1...|      CMT|        1|                 N| 07-01-13 06:08|  07-01-13 06:13|              1|       -73.96212|      40.769737|       -73.979561|        40.75539|\n",
      "|0C5296F3C8B16E702...|D2363240A9295EF57...|      CMT|        1|                 N| 07-01-13 22:25|  07-01-13 22:36|              1|      -73.989708|      40.756714|       -73.977615|       40.787575|\n",
      "+--------------------+--------------------+---------+---------+------------------+---------------+----------------+---------------+----------------+---------------+-----------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rides_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04f45a9-6cb5-4700-8537-1bed3a02d5a7",
   "metadata": {},
   "source": [
    "## Query 1\n",
    "\n",
    "In this query we calculate utilization per taxi/driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef506214-8532-49f2-a3c4-7f1bac4ddd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------+------------+\n",
      "|pickup_datetime|dropoff_datetime|duration_sec|\n",
      "+---------------+----------------+------------+\n",
      "| 01-01-13 15:11|  01-01-13 15:18|         420|\n",
      "| 06-01-13 00:18|  06-01-13 00:22|         240|\n",
      "| 05-01-13 18:49|  05-01-13 18:54|         300|\n",
      "| 07-01-13 23:54|  07-01-13 23:58|         240|\n",
      "| 07-01-13 23:25|  07-01-13 23:34|         540|\n",
      "| 07-01-13 15:27|  07-01-13 15:38|         660|\n",
      "| 08-01-13 11:01|  08-01-13 11:08|         420|\n",
      "| 07-01-13 12:39|  07-01-13 13:10|        1860|\n",
      "| 07-01-13 18:15|  07-01-13 18:20|         300|\n",
      "| 07-01-13 15:33|  07-01-13 15:49|         960|\n",
      "| 08-01-13 13:11|  08-01-13 13:19|         480|\n",
      "| 08-01-13 09:50|  08-01-13 10:02|         720|\n",
      "| 10-01-13 12:07|  10-01-13 12:17|         600|\n",
      "| 07-01-13 07:35|  07-01-13 07:46|         660|\n",
      "| 10-01-13 15:42|  10-01-13 16:04|        1320|\n",
      "| 10-01-13 14:27|  10-01-13 14:45|        1080|\n",
      "| 07-01-13 22:09|  07-01-13 22:19|         600|\n",
      "| 07-01-13 17:18|  07-01-13 17:20|         120|\n",
      "| 07-01-13 06:08|  07-01-13 06:13|         300|\n",
      "| 07-01-13 22:25|  07-01-13 22:36|         660|\n",
      "+---------------+----------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the unix_timestamp function to convert the date time of the respective times into milliseconds\n",
    "DATETIME_FORMAT = \"dd-MM-yy HH:mm\"\n",
    "\n",
    "rides_df = rides_df.withColumn(\"pickup_unix\", unix_timestamp(\"pickup_datetime\", DATETIME_FORMAT)) \\\n",
    "                   .withColumn(\"dropoff_unix\", unix_timestamp(\"dropoff_datetime\", DATETIME_FORMAT))\n",
    "\n",
    "# Subtract the times to find the difference (in seconds)\n",
    "rides_df = rides_df.withColumn(\"duration_sec\", rides_df[\"dropoff_unix\"] - rides_df[\"pickup_unix\"])\n",
    "\n",
    "# Output table with pickup time, dropoff time, and duration (seconds) for verification\n",
    "rides_df.select(\"pickup_datetime\", \"dropoff_datetime\", \"duration_sec\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f281be8-cc99-476f-830c-fd81b8689779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the difference between a ride and its subsequent ride is more than 4 hours, we ignore that as it represents a new session. \n",
    "# Differences below 4 hours is considered as an idle time\n",
    "DURATION_THRESHOLD = 14400 # 4 hours in seconds\n",
    "# Filter OUT rides longer than 4 hours from the dataset\n",
    "rides_df = rides_df.filter((col(\"duration_sec\") >= 0) & (col(\"duration_sec\") <= DURATION_THRESHOLD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e5e7533-a3c7-4236-9dea-20c75f230425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|        hack_license|total_idle_time_sec|\n",
      "+--------------------+-------------------+\n",
      "|001C8AAB90AEE49F3...|              12960|\n",
      "|0025133AD810DBE80...|               2400|\n",
      "|002C093A2CB9FD40C...|              15300|\n",
      "|00447A6197DBB329F...|              13440|\n",
      "|0046F1E91AA13DEDE...|               9960|\n",
      "|00567B1CBFD51DDFA...|              10080|\n",
      "|006114F940CB87B3A...|              24000|\n",
      "|006313464EC98A24B...|              31500|\n",
      "|006B6BD90C7B5C985...|               6180|\n",
      "|00711D0CC3FB5BC90...|               6000|\n",
      "|007357E7FFE212879...|              18660|\n",
      "|007439EEDB510EF82...|               3240|\n",
      "|007E686365B4421FB...|               3840|\n",
      "|00927C48BA4C1B2B1...|              14460|\n",
      "|00A2DC1380E44036A...|              11100|\n",
      "|00AE05F56D451E89E...|              22200|\n",
      "|00B442110FA2D04A1...|              10680|\n",
      "|00B7691D86D96AEBD...|              12120|\n",
      "|00BB5ECED533BF463...|              10380|\n",
      "|00BF52E4A8E6DBB01...|               9720|\n",
      "+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Re-declare 4 hour threshold\n",
    "DRIVER_SESSION_LENGTH = 14400 # 4 hours in seconds\n",
    "# Sort by driver ID and pickup time so trips for each driver are in chronological order \n",
    "rides_df = rides_df.orderBy(\"hack_license\", \"pickup_datetime\")\n",
    "\n",
    "# Separate data by driver and order by pickup time\n",
    "window_spec = Window.partitionBy(\"hack_license\").orderBy(\"pickup_unix\")\n",
    "\n",
    "# Include previous dropoff time and idle time (if idle time is < 4 hours, else 0)\n",
    "rides_df = rides_df.withColumn(\"prev_dropoff_unix\", lag(\"dropoff_unix\").over(window_spec)) \\\n",
    "                   .withColumn(\"idle_time_sec\", \n",
    "                               when((col(\"pickup_unix\") - col(\"prev_dropoff_unix\")) < DRIVER_SESSION_LENGTH,\n",
    "                                    col(\"pickup_unix\") - col(\"prev_dropoff_unix\"))\n",
    "                               .otherwise(0))\n",
    "\n",
    "# Filter out rows with 0 idle_time, group by driver id, sum total idle time for each driver\n",
    "idle_time_df = rides_df.filter(col(\"idle_time_sec\") > 0) \\\n",
    "                       .groupBy(\"hack_license\") \\\n",
    "                       .sum(\"idle_time_sec\") \\\n",
    "                       .withColumnRenamed(\"sum(idle_time_sec)\", \"total_idle_time_sec\")\n",
    "# Output table of total IDLE time per driver\n",
    "idle_time_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "227935fc-373f-4bf7-a8a2-b652fc09b35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|        hack_license|  utilization_rate|\n",
      "+--------------------+------------------+\n",
      "|751EE304AD669A5C9...|0.9885057471264368|\n",
      "|0D61304FB9E9E7CF6...|            0.9875|\n",
      "|3836463623075CCF1...|0.9850746268656716|\n",
      "|9BAB1E0D579D293F4...|0.9833333333333333|\n",
      "|B24D87EA86C349F5B...|0.9830508474576272|\n",
      "|06150B4FF9CD737D0...|0.9824561403508771|\n",
      "|31A1F192A01B30B2C...|0.9814814814814815|\n",
      "|7E73B38D829AA77F2...|0.9811320754716981|\n",
      "|1735076F27B86A649...|0.9807692307692307|\n",
      "|56F5F59C3EC8ACA1F...|0.9777777777777777|\n",
      "|958460D2C89BCF8C7...|0.9777777777777777|\n",
      "|BA510F229E9E7E292...|0.9772727272727273|\n",
      "|5DE0F6ED31F876A28...|             0.975|\n",
      "|B7F88F199CE4624DF...|0.9743589743589743|\n",
      "|D57EC7DC90ABE6176...|0.9743589743589743|\n",
      "|1603132156F27D303...|0.9736842105263158|\n",
      "|25D94112137704AE1...|0.9736842105263158|\n",
      "|B708C3D9C585DA845...|0.9736842105263158|\n",
      "|49365436007E31EE7...|0.9722222222222222|\n",
      "|AB2569908ED389C92...| 0.967741935483871|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calc total ACTIVE times by summing all ride durations (for each driver)\n",
    "active_time_df = rides_df.groupBy(\"hack_license\") \\\n",
    "                        .sum(\"duration_sec\") \\\n",
    "                        .withColumnRenamed(\"sum(duration_sec)\", \"total_active_time_sec\")\n",
    "\n",
    "# Join with IDLE time data\n",
    "driver_times_df = active_time_df.join(idle_time_df, \"hack_license\")\n",
    "\n",
    "# Calc total SESSION times by summing idle and ACTIVE times\n",
    "# Calc utilization rate difference: ACTIVE/SESSION\n",
    "utilization_df = driver_times_df.withColumn(\n",
    "    \"total_session_time\", \n",
    "    col(\"total_active_time_sec\") + col(\"total_idle_time_sec\")\n",
    ").withColumn(\n",
    "    \"utilization_rate\", \n",
    "    col(\"total_active_time_sec\") / col(\"total_session_time\")\n",
    ")\n",
    "\n",
    "# Output sorted table for verification\n",
    "utilization_df.select(\"hack_license\", \"utilization_rate\").orderBy(\"utilization_rate\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb7de16-849a-4a4c-be63-bb23b98e7791",
   "metadata": {},
   "source": [
    "## Query 2\n",
    "\n",
    "In this query we calculate the average time it takes for a taxi to find its next fare(trip) per destination borough.\n",
    "\n",
    "*The difference (in seconds) between the drop off of a trip and the pick up of the next trip*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0ebba5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df = (spark.read.option(\"multiline\", \"true\").json(\"input/nyc-boroughs.geojson\")).select(explode(\"features\").alias(\"borough\")) \\\n",
    "        .select(col(\"borough.geometry\").alias(\"geometry\"),\n",
    "                col(\"borough.properties.borough\").alias(\"borough_name\"),\n",
    "                col(\"borough.properties.boroughCode\").alias(\"borough_code\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ec3fa1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wkt serializes the geometry so we can store it as a string\n",
    "geo_to_poly_udf = udf(lambda geo: Polygon(geo[\"coordinates\"][0]).wkt, returnType=StringType())\n",
    "geo_to_area_udf = udf(lambda geo: Polygon(geo[\"coordinates\"][0]).area, returnType=DoubleType())\n",
    "coords_to_point_udf = udf(lambda coords: Point(coords[0], coords[1]).wkt, returnType=StringType()) # For the main dataframe\n",
    "is_point_in_polygon_udf = udf(lambda point_wkt, polygon_wkt: from_wkt(polygon_wkt).contains(from_wkt(point_wkt)), BooleanType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "84ed1b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df = geo_df.withColumn(\"poly\", geo_to_poly_udf(col(\"geometry\"))). \\\n",
    "        withColumn(\"area\", geo_to_area_udf(col(\"geometry\"))). \\\n",
    "        drop(\"geometry\"). \\\n",
    "        orderBy(\"borough_code\", desc(\"area\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ed95a2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- borough_name: string (nullable = true)\n",
      " |-- borough_code: long (nullable = true)\n",
      " |-- poly: string (nullable = true)\n",
      " |-- area: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "geo_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1377e030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+--------------------+--------------------+\n",
      "|borough_name|borough_code|                poly|                area|\n",
      "+------------+------------+--------------------+--------------------+\n",
      "|   Manhattan|           1|POLYGON ((-73.926...|0.005859077996035753|\n",
      "|   Manhattan|           1|POLYGON ((-73.921...|2.327165585676201...|\n",
      "|   Manhattan|           1|POLYGON ((-74.016...|  7.6037752599342E-5|\n",
      "|   Manhattan|           1|POLYGON ((-73.941...| 6.23157479510608E-5|\n",
      "|   Manhattan|           1|POLYGON ((-73.906...|3.265859127204495...|\n",
      "|   Manhattan|           1|POLYGON ((-74.039...|1.182888313767709...|\n",
      "|   Manhattan|           1|POLYGON ((-74.043...|6.143638903459381E-6|\n",
      "|   Manhattan|           1|POLYGON ((-73.995...|3.383127367444441...|\n",
      "|   Manhattan|           1|POLYGON ((-74.001...|2.858823502476497E-6|\n",
      "|   Manhattan|           1|POLYGON ((-74.000...|2.393654308790746E-6|\n",
      "|   Manhattan|           1|POLYGON ((-74.001...|2.334554077223592...|\n",
      "|   Manhattan|           1|POLYGON ((-73.998...|2.262734097676114...|\n",
      "|   Manhattan|           1|POLYGON ((-74.003...|2.246529747538856...|\n",
      "|   Manhattan|           1|POLYGON ((-74.002...| 2.24116737819868E-6|\n",
      "|   Manhattan|           1|POLYGON ((-74.005...|2.209445592708930...|\n",
      "|   Manhattan|           1|POLYGON ((-73.997...|2.048636296050274...|\n",
      "|   Manhattan|           1|POLYGON ((-74.010...|1.330049477425571...|\n",
      "|   Manhattan|           1|POLYGON ((-73.938...|1.111177055165405...|\n",
      "|   Manhattan|           1|POLYGON ((-73.962...|2.354033827718773E-7|\n",
      "|   Manhattan|           1|POLYGON ((-73.963...|2.092902292672452...|\n",
      "+------------+------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "geo_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7c6291a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pickup and dropoff coordinates to points\n",
    "rides_df = rides_df.withColumn(\"pickup_point\", coords_to_point_udf(array(\"pickup_longitude\", \"pickup_latitude\"))) \\\n",
    "        .withColumn(\"dropoff_point\", coords_to_point_udf(array(\"dropoff_longitude\", \"dropoff_latitude\"))) \\\n",
    "        .drop(\"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b7c1562a-3089-4459-95b0-344636296f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------------+\n",
      "|dropoff_borough|avg_next_fare_time_sec|\n",
      "+---------------+----------------------+\n",
      "|      Manhattan|     899.7157508788288|\n",
      "|            N/A|    1484.9464558898521|\n",
      "|       Brooklyn|     2075.489067894131|\n",
      "|          Bronx|    2223.5643564356437|\n",
      "|         Queens|    2665.1269765213224|\n",
      "|  Staten Island|                4710.0|\n",
      "+---------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OPTIMIZATION: Broadcast geo data for quicker borough lookup\n",
    "borough_lookup = spark.sparkContext.broadcast(geo_df.collect())\n",
    "\n",
    "# Similar to the previously declared udf: is_point_in_polygon_udf\n",
    "# Some data does not fall into any borough (Unsure if data is relevant or error)\n",
    "# Grouped these outliers in \"N/A\"\n",
    "find_borough_udf = udf(lambda point_wkt: \\\n",
    "    next((b['borough_name'] for b in borough_lookup.value \\\n",
    "          if point_wkt and from_wkt(b['poly']).contains(from_wkt(point_wkt))), \\\n",
    "         \"N/A\"), \\\n",
    "    StringType())\n",
    "\n",
    "# Add pickup + dropoff borough columns\n",
    "rides_df = rides_df.withColumn(\"pickup_borough\", find_borough_udf(col(\"pickup_point\"))) \\\n",
    "                   .withColumn(\"dropoff_borough\", find_borough_udf(col(\"dropoff_point\")))\n",
    "\n",
    "# Separate data by driver and order by pickup time\n",
    "window_spec = Window.partitionBy(\"hack_license\").orderBy(\"pickup_unix\")\n",
    "# Calc next fare time by borough - Filter out nulls and 4 hours\n",
    "next_fare_df = rides_df.withColumn(\"next_pickup_unix\", \n",
    "                                  lead(\"pickup_unix\").over(window_spec)) \\\n",
    "                       .withColumn(\"next_fare_time\", \n",
    "                                  when(col(\"next_pickup_unix\").isNotNull(), \n",
    "                                       col(\"next_pickup_unix\") - col(\"dropoff_unix\"))\n",
    "                                  .otherwise(None)) \\\n",
    "                       .filter(col(\"next_fare_time\").isNotNull() & \n",
    "                              (col(\"next_fare_time\") <= DRIVER_SESSION_LENGTH))\n",
    "\n",
    "# Calc avg. next fare time by dropoff borough and sort\n",
    "avg_next_fare_by_borough = next_fare_df.groupBy(\"dropoff_borough\") \\\n",
    "                                       .agg(avg(\"next_fare_time\").alias(\"avg_next_fare_time_sec\")) \\\n",
    "                                       .orderBy(\"avg_next_fare_time_sec\")\n",
    "\n",
    "# Show results\n",
    "avg_next_fare_by_borough.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26061ba-1aee-495f-b879-772cac80a188",
   "metadata": {},
   "source": [
    "## Query 3\n",
    "\n",
    "The number of trips that started and ended within the same borough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d48a4917",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'broadcast' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Query 3. The number of trips that started and ended within the same borough\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m geo_broadcast \u001b[38;5;241m=\u001b[39m \u001b[43mbroadcast\u001b[49m(geo_df)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Find pickup borough for each ride\u001b[39;00m\n\u001b[1;32m      5\u001b[0m pickup_borough \u001b[38;5;241m=\u001b[39m rides_df \\\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39mcrossJoin(geo_broadcast) \\\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39mfilter(is_point_in_polygon_udf(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpickup_point\u001b[39m\u001b[38;5;124m\"\u001b[39m), col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpoly\u001b[39m\u001b[38;5;124m\"\u001b[39m))) \\\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m         col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mborough_code\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpickup_borough_code\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'broadcast' is not defined"
     ]
    }
   ],
   "source": [
    "# Query 3. The number of trips that started and ended within the same borough\n",
    "geo_broadcast = broadcast(geo_df)\n",
    "\n",
    "# Find pickup borough for each ride\n",
    "pickup_borough = rides_df \\\n",
    "    .crossJoin(geo_broadcast) \\\n",
    "    .filter(is_point_in_polygon_udf(col(\"pickup_point\"), col(\"poly\"))) \\\n",
    "    .select(\n",
    "        rides_df[\"medallion\"], \n",
    "        rides_df[\"hack_license\"], \n",
    "        rides_df[\"pickup_datetime\"],\n",
    "        rides_df[\"dropoff_point\"],\n",
    "        col(\"borough_name\").alias(\"pickup_borough\"),\n",
    "        col(\"borough_code\").alias(\"pickup_borough_code\")\n",
    "    )\n",
    "\n",
    "# Find dropoff borough for each ride\n",
    "dropoff_borough = rides_df \\\n",
    "    .crossJoin(geo_broadcast) \\\n",
    "    .filter(is_point_in_polygon_udf(col(\"dropoff_point\"), col(\"poly\"))) \\\n",
    "    .select(\n",
    "        rides_df[\"medallion\"], \n",
    "        rides_df[\"hack_license\"], \n",
    "        rides_df[\"pickup_datetime\"],\n",
    "        col(\"borough_name\").alias(\"dropoff_borough\"),\n",
    "        col(\"borough_code\").alias(\"dropoff_borough_code\")\n",
    "    )\n",
    "\n",
    "# Join the two DFs to find same-borough trips\n",
    "same_borough_trips = pickup_borough \\\n",
    "    .join(\n",
    "        dropoff_borough, \n",
    "        [\"medallion\", \"hack_license\", \"pickup_datetime\"],\n",
    "        \"inner\"\n",
    "    ) \\\n",
    "    .filter(col(\"pickup_borough_code\") == col(\"dropoff_borough_code\"))\n",
    "\n",
    "# Count trips by borough\n",
    "borough_counts = same_borough_trips \\\n",
    "    .groupBy(\"pickup_borough\", \"pickup_borough_code\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"pickup_borough_code\")\n",
    "\n",
    "# Get total\n",
    "total_same_borough = same_borough_trips.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8715c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_same_borough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c1cf21-70a7-42b6-8313-55850557c9a8",
   "metadata": {},
   "source": [
    "## Query 4\n",
    "\n",
    "The number of trips that started and ended within the same borough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee93f949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 4. The number of trips that started in one borough and ended in another one\n",
    "\n",
    "rides_df.count() - total_same_borough"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
