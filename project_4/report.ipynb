{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38676564",
   "metadata": {},
   "source": [
    "# BigData 2025: Project 4\n",
    "Students: Kalju Jake Nekvasil, Joosep Orasm√§e, Tanel Tiisler, Kaupo Humal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3025a497",
   "metadata": {},
   "source": [
    "## Data Ingestion and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6f535b",
   "metadata": {},
   "source": [
    "## Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bde31c",
   "metadata": {},
   "source": [
    "## Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849c8cb9",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71c6ac6",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "We trained 4 different models as required by the task: LogisticRegression, DecisionTree, RandomForest and GBT. Due to computational complexity and time constraints, we ran CrossValidation for each of the models for just a single hyperparameter, testing 3 different values.\n",
    "\n",
    "All of the models had very similar accuracy but differing AUC scores. As the GBT model had the highest AUC, we chose that one as our model of choice.\n",
    "\n",
    "The GBT model does have a big downside of being more computationally expensive, since it relies on sequential operations for training. This meant that we could only do a very limited hyperparameter search in a reasonable amount of runtime. Thankfully inference is still very fast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8bc818",
   "metadata": {},
   "source": [
    "## Explainability\n",
    "\n",
    "### Top 10 Features by Importance\n",
    "\n",
    "| Rank | Feature Index | Decoded Feature            | Importance Score | Interpretation |\n",
    "|------|---------------|----------------------------|------------------|----------------|\n",
    "| 1    | -             | **DepDelay**               | 0.5778           | Departure delay |\n",
    "| 2    | 633           | Month_9 (September)        | 0.0454           | Peak hurricane season |\n",
    "| 3    | 625           | Month_1 (January)          | 0.0357           | Winter storms |\n",
    "| 4    | 628           | Month_4 (April)            | 0.0311           | Spring break travel disruptions |\n",
    "| 5    | 632           | Month_8 (August)           | 0.0284           | Summer travel peaks |\n",
    "| 6    | 607           | UniqueCarrier_FL (AirTran) | 0.0259           | Out-of-business carrier |\n",
    "| 7    | -             | **Distance**               | 0.0253           | Longer flight risk |\n",
    "| 8    | 317           | Dest_ALO (Waterloo, IA)    | 0.0249           | Small Airport, Unpredictable Midwest weather |\n",
    "| 9    | 26            | Origin_ATW (Appleton, WI)  | 0.0244           | Small Airport, Unpredicatable Midwest Weather |\n",
    "| 10   | 304           | Dest_ABI (Abilene, TX)     | 0.0242           | Small Airport, Extreme Texas Weather |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6357c0a2",
   "metadata": {},
   "source": [
    "## Model Persistence and Inference\n",
    "\n",
    "SparkML makes combining different processing steps very easy, as everything is basically a transformer. We also made a Preprocessor class that has a `_transform(self, df)` method, allowing it to be used as a pipeline stage. This means the we can combine everything into one big pipeline such that the raw dataframe goes in on one end and predictions come out the other.\n",
    "\n",
    "Testing this full pipeline on the 2010.csv dataset we achieved an accuracy score of <b>0.9824</b> and an AUC score of <b>0.9555</b>."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
