{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "648f3b47-dcc8-42d3-90e9-b7b31f64d995",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType, LongType\n",
    "from delta import *\n",
    "import os\n",
    "import time\n",
    "# !pip install delta-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe2f2b4c-08bd-4de6-b8ea-e1226b31049a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkSession with Delta Lake support\n",
    "builder = SparkSession.builder.appName(\"SensorDataWindow\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.driver.memory\", \"5g\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8a4b9dd-a852-4d61-9eef-f62067969389",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"medallion\", StringType(), True),\n",
    "    StructField(\"hack_license\", StringType(), True),\n",
    "    StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "    StructField(\"dropoff_datetime\", TimestampType(), True),\n",
    "    StructField(\"trip_time_in_secs\", IntegerType(), True),\n",
    "    StructField(\"trip_distance\", DoubleType(), True),\n",
    "    StructField(\"pickup_longitude\", DoubleType(), True),\n",
    "    StructField(\"pickup_latitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_longitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_latitude\", DoubleType(), True),\n",
    "    StructField(\"payment_type\", StringType(), True),\n",
    "    StructField(\"fare_amount\", DoubleType(), True),\n",
    "    StructField(\"surcharge\", DoubleType(), True),\n",
    "    StructField(\"mta_tax\", DoubleType(), True),\n",
    "    StructField(\"tip_amount\", DoubleType(), True),\n",
    "    StructField(\"tolls_amount\", DoubleType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1d6c96",
   "metadata": {},
   "source": [
    "# Query 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "461072d8-e2fb-4ab6-84ce-4dbf0bce635d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#original, unmodified dataset\n",
    "rides_df = (spark.read\n",
    "            .schema(schema)\n",
    "            .csv(\"input/minified_sorted_data.csv\") #path\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1009cab5-4810-45ef-a210-23898c631c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- medallion: string (nullable = true)\n",
      " |-- hack_license: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- trip_time_in_secs: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- surcharge: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rides_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c8fda01a-bf51-4eff-936d-6fd75bdf59d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 lines\n"
     ]
    }
   ],
   "source": [
    "# remove rows with null values\n",
    "initial_count = rides_df.count()\n",
    "rides_df = rides_df.dropna()\n",
    "\n",
    "print(f\"Removed {initial_count - rides_df.count()} lines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4d2509d4-87da-4bf5-90f4-0217286535fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 lines\n"
     ]
    }
   ],
   "source": [
    "# check if medallion and hash license are valid md5, remove the row otherwise\n",
    "\n",
    "MD5_PATTERN = r\"^[a-fA-F0-9]{32}$\"\n",
    "\n",
    "initial_count = rides_df.count()\n",
    "\n",
    "rides_df = rides_df.filter(\n",
    "    col(\"medallion\").rlike(MD5_PATTERN) &\n",
    "    col(\"hack_license\").rlike(MD5_PATTERN)\n",
    ")\n",
    "\n",
    "print(f\"Removed {initial_count - rides_df.count()} lines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1500f9be-3e5c-4093-b59e-f010f9253732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 266 lines\n"
     ]
    }
   ],
   "source": [
    "# remove rows with invalid pickup of dropoff times\n",
    "\n",
    "initial_count = rides_df.count()\n",
    "\n",
    "rides_df = rides_df.filter(\n",
    "    col(\"dropoff_datetime\") > col(\"pickup_datetime\")\n",
    ")\n",
    "\n",
    "print(f\"Removed {initial_count - rides_df.count()} lines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b9c922cf-4e2f-43a6-a988-09522dd5a473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 319 lines\n"
     ]
    }
   ],
   "source": [
    "# check for illogical numeric values\n",
    "\n",
    "initial_count = rides_df.count()\n",
    "\n",
    "rides_df = rides_df.filter(\n",
    "    (col(\"trip_time_in_secs\") > 0) &\n",
    "    (col(\"trip_distance\") > 0) &\n",
    "    (col(\"fare_amount\") >= 0) &\n",
    "    (col(\"surcharge\") >= 0) &\n",
    "    (col(\"mta_tax\") >= 0) &\n",
    "    (col(\"tip_amount\") >= 0) &\n",
    "    (col(\"tolls_amount\") >= 0)\n",
    ")\n",
    "\n",
    "print(f\"Removed {initial_count - rides_df.count()} lines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b0cecd16-3fd9-4bba-9a1d-5245e9285855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 845 lines\n"
     ]
    }
   ],
   "source": [
    "# remove lines with invalid fare calculation, eg where total_amount does not equal the sum of all fees\n",
    "\n",
    "initial_count = rides_df.count()\n",
    "\n",
    "rides_df = rides_df.filter(\n",
    "    col(\"total_amount\") ==\n",
    "    (\n",
    "        col(\"fare_amount\") + \n",
    "        col(\"surcharge\") + \n",
    "        col(\"mta_tax\") + \n",
    "        col(\"tip_amount\") + \n",
    "        col(\"tolls_amount\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Removed {initial_count - rides_df.count()} lines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "315fe014-569e-4090-9c07-d50da11e59a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73570"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rides_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9b209707",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_df_1000 = rides_df.limit(1000)\n",
    "\n",
    "rides_df_1000.write.csv(\"input/rides_df_1000\", header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fb22808d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the cleaned dataset to file\n",
    "rides_df.write.csv(\"input/cleaned_minified_data\", header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b88175",
   "metadata": {},
   "source": [
    "# Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "412743bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For inspecting the data\n",
    "rides_df = (spark.read\n",
    "            .schema(schema)\n",
    "            .option(\"header\", \"true\")\n",
    "            .csv(\"input/rides_df_1000\") #path\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc54abcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+--------------------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "|medallion                       |hack_license                    |pickup_datetime    |dropoff_datetime   |trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|total_amount|\n",
      "+--------------------------------+--------------------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "|5EE2C4D3BF57BDB455E74B03B89E43A7|E96EF8F6E6122591F9465376043B946D|2013-01-01 00:00:09|2013-01-01 00:00:36|26               |0.1          |-73.99221       |40.725124      |-73.991646       |40.726658       |CSH         |2.5        |0.5      |0.5    |0.0       |0.0         |3.5         |\n",
      "|CA6CD9BAED6A85E430F7BFC0BC84ABD0|77FFDF38272A6006517D53EDA14333E2|2013-01-01 00:00:20|2013-01-01 00:01:22|61               |2.2          |-73.9701        |40.768005      |-73.969772       |40.767834       |CSH         |3.0        |0.5      |0.5    |0.0       |0.0         |4.0         |\n",
      "|15162141EA7436635C696F5BC023D2D6|CDCB7729DE07243726FF7BB0BD5D06BF|2013-01-01 00:00:14|2013-01-01 00:01:37|83               |0.2          |-73.975441      |40.749657      |-73.977333       |40.751991       |CSH         |3.0        |0.5      |0.5    |0.0       |0.0         |4.0         |\n",
      "|025B98A22ED771118FC0EB44A0D3BD9D|7D89374F8E98F30A19F2381EC71A16BA|2013-01-01 00:00:40|2013-01-01 00:01:40|60               |0.3          |-74.005165      |40.720531      |-74.003929       |40.725655       |CSH         |3.0        |0.5      |0.5    |0.0       |0.0         |4.0         |\n",
      "|07290D3599E7A0D62097A346EFCC1FB5|E7750A37CAB07D0DFF0AF7E3573AC141|2013-01-01 00:00:00|2013-01-01 00:02:00|120              |0.44         |-73.956528      |40.716976      |-73.96244        |40.715008       |CSH         |3.5        |0.5      |0.5    |0.0       |0.0         |4.5         |\n",
      "+--------------------------------+--------------------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rides_df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eba487dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_stream = (spark.readStream\n",
    "                .schema(schema)\n",
    "                .option(\"header\", \"true\")\n",
    "                .csv(\"input/cleaned_minified_data\") #path\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "563a14b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# UDF for converting latitude and longitude to a grid cell ID\n",
    "# Courtesy of Claude 3.7\n",
    "def lat_long_to_grid(lat, long):\n",
    "    # Barryville reference point (center of cell 1.1)\n",
    "    reference_lat = 41.474937\n",
    "    reference_long = -74.913585\n",
    "    \n",
    "    # Calculate distance from reference point\n",
    "    # For latitude: 1 degree ~ 111 km (varies slightly with latitude)\n",
    "    # Moving south means decreasing latitude\n",
    "    lat_dist_km = (reference_lat - lat) * 111.0  # Distance south in km\n",
    "    \n",
    "    # For longitude: 1 degree ~ 111 * cos(latitude) km\n",
    "    # Moving east means increasing longitude\n",
    "    long_dist_km = (long - reference_long) * 111.0 * math.cos(math.radians(reference_lat))  # Distance east in km\n",
    "    \n",
    "    # Convert to meters\n",
    "    lat_dist_m = lat_dist_km * 1000\n",
    "    long_dist_m = long_dist_km * 1000\n",
    "    \n",
    "    # Check if outside the grid (more than 150km south or east from reference)\n",
    "    if lat_dist_m < 0 or lat_dist_m > 150000 or long_dist_m < 0 or long_dist_m > 150000:\n",
    "        return None\n",
    "    \n",
    "    # Calculate cell IDs\n",
    "    # Cell 1.1 starts at reference point (center of the cell)\n",
    "    # Each cell is 500m x 500m\n",
    "    # To get the cell number, divide by 500 and add 1\n",
    "    \n",
    "    # For the first component (east direction)\n",
    "    cell_east = int(long_dist_m / 500) + 1\n",
    "    \n",
    "    # For the second component (south direction)\n",
    "    cell_south = int(lat_dist_m / 500) + 1\n",
    "    \n",
    "    # Cell ID as \"east.south\"\n",
    "    return f\"{cell_east}.{cell_south}\"\n",
    "\n",
    "# Register the UDF with Spark\n",
    "lat_long_to_grid_udf = udf(lat_long_to_grid, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07130aec",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08cd5815",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_routes_query = (rides_stream\n",
    "    .withColumn(\"start_cell\", lat_long_to_grid_udf(col(\"pickup_latitude\"), col(\"pickup_longitude\")))\n",
    "    .withColumn(\"end_cell\", lat_long_to_grid_udf(col(\"dropoff_latitude\"), col(\"dropoff_longitude\")))\n",
    "    .filter(col(\"start_cell\").isNotNull() & col(\"end_cell\").isNotNull()) # The udf returns None for invalid cells\n",
    "    .groupBy(\n",
    "        window(col(\"pickup_datetime\"), \"30 minutes\"),\n",
    "        col(\"start_cell\"),\n",
    "        col(\"end_cell\")\n",
    "    )\n",
    "    .agg(count(\"*\").alias(\"Number_of_Rides\"))\n",
    "    .orderBy(col(\"Number_of_Rides\").desc())\n",
    "    .limit(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f0852c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create table if not exists\n",
    "def create_table_if_exists(output_path, table_name):\n",
    "    data_exists = False\n",
    "    for _i in range(5):  # Retry for 60 seconds\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            files = os.listdir(output_path)\n",
    "            for _f in files:\n",
    "                if \".parquet\" in _f:\n",
    "                    if len(os.listdir(f\"{output_path}/_delta_log\")) > 0:\n",
    "                        print(\"data exists\")\n",
    "                        data_exists = True\n",
    "                        break\n",
    "            if data_exists:\n",
    "                spark.sql(f\"CREATE TABLE IF NOT EXISTS {table_name} USING DELTA LOCATION '{output_path}'\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(e)  # Uncomment if you want to see exceptions\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acbb7e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 718, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 16\u001b[0m\n\u001b[1;32m      5\u001b[0m table_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmost_frequent_routes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m create_table_if_exists(output_path, table_name)\n\u001b[1;32m      8\u001b[0m (\u001b[43mmost_frequent_routes_query\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriteStream\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputMode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpointLocation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqueryName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmost_frequent_routes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrigger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessingTime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m5 seconds\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m120\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming/query.py:219\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(timeout, (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m)) \u001b[38;5;129;01mor\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m    216\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVALUE_NOT_POSITIVE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    217\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_value\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(timeout)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    218\u001b[0m         )\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination()\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/socket.py:718\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 718\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    720\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"output/_checkpoint\"\n",
    "output_path = \"output/most_frequent_routes\"\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "table_name = \"most_frequent_routes\"\n",
    "create_table_if_exists(output_path, table_name)\n",
    "\n",
    "(most_frequent_routes_query.\n",
    "    writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"complete\")\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .queryName(\"most_frequent_routes\")\n",
    "    .trigger(processingTime=\"5 seconds\")\n",
    "    .start(output_path)\n",
    "    .awaitTermination(timeout = 120)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70670eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping existing query: most_frequent_routes\n"
     ]
    }
   ],
   "source": [
    "# Check for active streams and stop them if they exist\n",
    "for query in spark.streams.active:\n",
    "    if query.name.startswith(\"most_frequent_routes\"):\n",
    "        print(f\"Stopping existing query: {query.name}\")\n",
    "        query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b09acd23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>window</th>\n",
       "      <th>start_cell</th>\n",
       "      <th>end_cell</th>\n",
       "      <th>Number_of_Rides</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2711708</th>\n",
       "      <td>(2013-01-01 00:00:00, 2013-01-01 00:30:00)</td>\n",
       "      <td>155.166</td>\n",
       "      <td>173.161</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2320583</th>\n",
       "      <td>(2013-01-01 00:00:00, 2013-01-01 00:30:00)</td>\n",
       "      <td>157.163</td>\n",
       "      <td>152.168</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4136225</th>\n",
       "      <td>(2013-01-01 00:00:00, 2013-01-01 00:30:00)</td>\n",
       "      <td>160.156</td>\n",
       "      <td>157.162</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3103709</th>\n",
       "      <td>(2013-01-01 00:00:00, 2013-01-01 00:30:00)</td>\n",
       "      <td>162.152</td>\n",
       "      <td>158.149</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3378809</th>\n",
       "      <td>(2013-01-01 00:00:00, 2013-01-01 00:30:00)</td>\n",
       "      <td>151.166</td>\n",
       "      <td>152.166</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3103654</th>\n",
       "      <td>(2013-01-01 00:00:00, 2013-01-01 00:30:00)</td>\n",
       "      <td>157.159</td>\n",
       "      <td>156.166</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338437</th>\n",
       "      <td>(2013-01-01 00:00:00, 2013-01-01 00:30:00)</td>\n",
       "      <td>157.160</td>\n",
       "      <td>162.160</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2552291</th>\n",
       "      <td>(2013-01-01 00:00:00, 2013-01-01 00:30:00)</td>\n",
       "      <td>151.163</td>\n",
       "      <td>160.147</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3570852</th>\n",
       "      <td>(2013-01-01 00:00:00, 2013-01-01 00:30:00)</td>\n",
       "      <td>151.163</td>\n",
       "      <td>158.169</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2320450</th>\n",
       "      <td>(2013-01-01 00:00:00, 2013-01-01 00:30:00)</td>\n",
       "      <td>160.155</td>\n",
       "      <td>161.153</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             window start_cell end_cell  \\\n",
       "2711708  (2013-01-01 00:00:00, 2013-01-01 00:30:00)    155.166  173.161   \n",
       "2320583  (2013-01-01 00:00:00, 2013-01-01 00:30:00)    157.163  152.168   \n",
       "4136225  (2013-01-01 00:00:00, 2013-01-01 00:30:00)    160.156  157.162   \n",
       "3103709  (2013-01-01 00:00:00, 2013-01-01 00:30:00)    162.152  158.149   \n",
       "3378809  (2013-01-01 00:00:00, 2013-01-01 00:30:00)    151.166  152.166   \n",
       "3103654  (2013-01-01 00:00:00, 2013-01-01 00:30:00)    157.159  156.166   \n",
       "338437   (2013-01-01 00:00:00, 2013-01-01 00:30:00)    157.160  162.160   \n",
       "2552291  (2013-01-01 00:00:00, 2013-01-01 00:30:00)    151.163  160.147   \n",
       "3570852  (2013-01-01 00:00:00, 2013-01-01 00:30:00)    151.163  158.169   \n",
       "2320450  (2013-01-01 00:00:00, 2013-01-01 00:30:00)    160.155  161.153   \n",
       "\n",
       "         Number_of_Rides  \n",
       "2711708                1  \n",
       "2320583                1  \n",
       "4136225                3  \n",
       "3103709                1  \n",
       "3378809                2  \n",
       "3103654                1  \n",
       "338437                 1  \n",
       "2552291                1  \n",
       "3570852                2  \n",
       "2320450                1  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(output_path)\n",
    "df.toPandas().sort_values(by=['window'], ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b37ad75",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a58c1ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path_2 = \"output/_checkpoint_part2\"\n",
    "output_path_2 = \"output/most_frequent_routes_part2\"\n",
    "os.makedirs(output_path_2, exist_ok=True)\n",
    "os.makedirs(checkpoint_path_2, exist_ok=True)\n",
    "\n",
    "table_name_2 = \"most_frequent_routes_part2\"\n",
    "create_table_if_exists(output_path_2, table_name_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e422d0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_stream_2 = (spark.readStream\n",
    "                .schema(schema)\n",
    "                .option(\"header\", \"true\")\n",
    "                .option(\"maxFilesPerTrigger\", 1) #process one file at a time\n",
    "                .csv(\"input/cleaned_minified_data\"))\n",
    "\n",
    "# Include the ingestion time\n",
    "rides_stream_2 = rides_stream_2.withColumn(\"ingestion_time\", current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "df6867d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pickup and dropoff to grid IDs\n",
    "rides_stream_2 = (rides_stream_2\n",
    "                .withColumn(\"start_cell\", lat_long_to_grid_udf(col(\"pickup_latitude\"), col(\"pickup_longitude\")))\n",
    "                .withColumn(\"end_cell\", lat_long_to_grid_udf(col(\"dropoff_latitude\"), col(\"dropoff_longitude\")))\n",
    "                .filter(col(\"start_cell\").isNotNull() & col(\"end_cell\").isNotNull())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8a99ae1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add windowing and group together rides on the same route\n",
    "rides_stream_2 = (rides_stream_2\n",
    "                .groupBy(\n",
    "                    window(col(\"pickup_datetime\"), \"30 minutes\"),\n",
    "                    col(\"start_cell\"),\n",
    "                    col(\"end_cell\")\n",
    "                )\n",
    "                .agg(\n",
    "                    count(\"*\").alias(\"Number_of_Rides\"), \n",
    "                    min(col(\"ingestion_time\")).alias(\"ingestion_time\")\n",
    "                )\n",
    "                .orderBy(col(\"Number_of_Rides\").desc())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2ef3c857",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_schema = StructType([\n",
    "    StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "    StructField(\"dropoff_datetime\", TimestampType(), True),\n",
    "    StructField(\"start_cell_id_1\", StringType(), True),\n",
    "    StructField(\"end_cell_id_1\", StringType(), True),\n",
    "    StructField(\"start_cell_id_2\", StringType(), True),\n",
    "    StructField(\"end_cell_id_2\", StringType(), True),\n",
    "    StructField(\"start_cell_id_3\", StringType(), True),\n",
    "    StructField(\"end_cell_id_3\", StringType(), True),\n",
    "    StructField(\"start_cell_id_4\", StringType(), True),\n",
    "    StructField(\"end_cell_id_4\", StringType(), True),\n",
    "    StructField(\"start_cell_id_5\", StringType(), True),\n",
    "    StructField(\"end_cell_id_5\", StringType(), True),\n",
    "    StructField(\"start_cell_id_6\", StringType(), True),\n",
    "    StructField(\"end_cell_id_6\", StringType(), True),\n",
    "    StructField(\"start_cell_id_7\", StringType(), True),\n",
    "    StructField(\"end_cell_id_7\", StringType(), True),\n",
    "    StructField(\"start_cell_id_8\", StringType(), True),\n",
    "    StructField(\"end_cell_id_8\", StringType(), True),\n",
    "    StructField(\"start_cell_id_9\", StringType(), True),\n",
    "    StructField(\"end_cell_id_9\", StringType(), True),\n",
    "    StructField(\"start_cell_id_10\", StringType(), True),\n",
    "    StructField(\"end_cell_id_10\", StringType(), True),\n",
    "    StructField(\"delay\", LongType(), True)\n",
    "])\n",
    "\n",
    "previous_top10 = None\n",
    "\n",
    "def process_batch(batch_df, batch_id):\n",
    "    global previous_top10\n",
    "    \n",
    "    if batch_df.isEmpty():\n",
    "        return\n",
    "    \n",
    "    # Compare the current batch with the previous one\n",
    "    current_top10 = batch_df.orderBy(col(\"Number_of_Rides\").desc()).limit(10).collect()\n",
    "\n",
    "    if previous_top10 is None or current_top10 != previous_top10:\n",
    "        previous_top10 = current_top10\n",
    "\n",
    "        window_data = batch_df.select(\"window\").first()\n",
    "        if window_data:\n",
    "            # Create a row with explicit types\n",
    "            result = spark.createDataFrame(\n",
    "                [(\n",
    "                    window_data[\"window\"].start,  # pickup_datetime\n",
    "                    window_data[\"window\"].end,    # dropoff_datetime\n",
    "                    None, None, None, None, None, None, None, None, None, None,\n",
    "                    None, None, None, None, None, None, None, None, None, None,\n",
    "                    0  # placeholder for delay\n",
    "                )], \n",
    "                schema=result_schema  # Use the explicit schema\n",
    "            )\n",
    "            \n",
    "            # Populate the result with actual top 10 routes\n",
    "            for i, route in enumerate(current_top10):\n",
    "                result = result.withColumn(f\"start_cell_id_{i+1}\", lit(route[\"start_cell\"]))\n",
    "                result = result.withColumn(f\"end_cell_id_{i+1}\", lit(route[\"end_cell\"]))\n",
    "            \n",
    "            # Calculate delay in milliseconds\n",
    "            min_time_row = batch_df.agg(min(\"ingestion_time\").alias(\"min_time\")).first()\n",
    "            if min_time_row and min_time_row[\"min_time\"]:\n",
    "                earliest_processing_time = int(min_time_row[\"min_time\"].timestamp())\n",
    "                current_time = int(time.time())\n",
    "                delay_ms = (current_time - earliest_processing_time) * 1000\n",
    "                result = result.withColumn(\"delay\", lit(delay_ms))\n",
    "            \n",
    "            # Write to Delta table\n",
    "            result.write.format(\"delta\").mode(\"append\").save(output_path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "07bcd2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 718, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 16\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopping existing query: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m         query\u001b[38;5;241m.\u001b[39mstop()\n\u001b[1;32m      6\u001b[0m (\u001b[43mrides_stream_2\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeachBatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputMode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpointLocation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path_2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmergeSchema\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqueryName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmost_frequent_routes_part2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrigger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessingTime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m5 seconds\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_path_2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m120\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming/query.py:219\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(timeout, (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m)) \u001b[38;5;129;01mor\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m    216\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVALUE_NOT_POSITIVE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    217\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_value\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(timeout)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    218\u001b[0m         )\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination()\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/socket.py:718\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 718\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    720\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for query in spark.streams.active:\n",
    "    if query.name.startswith(\"most_frequent_routes\"):\n",
    "        print(f\"Stopping existing query: {query.name}\")\n",
    "        query.stop()\n",
    "\n",
    "(rides_stream_2\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .foreachBatch(process_batch)\n",
    "    .outputMode(\"complete\")\n",
    "    .option(\"checkpointLocation\", checkpoint_path_2)\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .queryName(\"most_frequent_routes_part2\")\n",
    "    .trigger(processingTime=\"5 seconds\")\n",
    "    .start(output_path_2)\n",
    "    .awaitTermination(timeout = 120)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7184d0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+----------------+--------------+------+\n",
      "|pickup_datetime    |dropoff_datetime   |start_cell_id_1|end_cell_id_1|start_cell_id_2|end_cell_id_2|start_cell_id_3|end_cell_id_3|start_cell_id_4|end_cell_id_4|start_cell_id_5|end_cell_id_5|start_cell_id_6|end_cell_id_6|start_cell_id_7|end_cell_id_7|start_cell_id_8|end_cell_id_8|start_cell_id_9|end_cell_id_9|start_cell_id_10|end_cell_id_10|delay |\n",
      "+-------------------+-------------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+----------------+--------------+------+\n",
      "|2013-01-14 08:00:00|2013-01-14 08:30:00|155.159        |152.160      |155.159        |152.160      |155.159        |152.160      |155.159        |153.159      |155.159        |152.160      |155.159        |152.160      |155.159        |152.160      |155.159        |152.160      |154.160        |155.159      |155.159         |152.160       |639000|\n",
      "+-------------------+-------------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+----------------+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_df = spark.read.format(\"delta\").load(output_path_2)\n",
    "\n",
    "results_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7b23bada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping existing query: most_frequent_routes_part2\n",
      "Starting real-time monitoring\n",
      "\n",
      "==================================================\n",
      "NEW UPDATE DETECTED at 17:53:36\n",
      "Window: 2013-01-08 06:30:00 to 2013-01-08 07:00:00\n",
      "Processing delay: 21000 ms\n",
      "--------------------------------------------------\n",
      "Top 10 Routes:\n",
      "  #1: 154.160 → 155.159\n",
      "  #2: 154.160 → 155.159\n",
      "  #3: 154.160 → 156.160\n",
      "  #4: 154.160 → 156.160\n",
      "  #5: 154.160 → 155.159\n",
      "  #6: 154.160 → 156.160\n",
      "  #7: 154.160 → 156.159\n",
      "  #8: 154.160 → 156.159\n",
      "  #9: 154.160 → 156.160\n",
      "  #10: 154.160 → 156.160\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "NEW UPDATE DETECTED at 17:53:36\n",
      "Window: 2013-01-17 07:30:00 to 2013-01-17 08:00:00\n",
      "Processing delay: 12000 ms\n",
      "--------------------------------------------------\n",
      "Top 10 Routes:\n",
      "  #1: 154.160 → 156.160\n",
      "  #2: 154.160 → 155.159\n",
      "  #3: 154.160 → 156.160\n",
      "  #4: 154.160 → 155.159\n",
      "  #5: 154.161 → 156.161\n",
      "  #6: 154.160 → 156.160\n",
      "  #7: 159.157 → 157.160\n",
      "  #8: 154.160 → 155.159\n",
      "  #9: 155.163 → 156.161\n",
      "  #10: 154.160 → 156.159\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 718, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Monitoring stopped.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "for query in spark.streams.active:\n",
    "    if query.name.startswith(\"most_frequent_routes\"):\n",
    "        print(f\"Stopping existing query: {query.name}\")\n",
    "        query.stop()\n",
    "\n",
    "# Start the stream in a separate cell (don't use awaitTermination)\n",
    "streaming_query = (rides_stream_2\n",
    "    .writeStream\n",
    "    .foreachBatch(process_batch)\n",
    "    .outputMode(\"complete\")\n",
    "    .option(\"checkpointLocation\", checkpoint_path_2)\n",
    "    .queryName(\"most_frequent_routes_part2\")\n",
    "    .trigger(processingTime=\"5 seconds\")\n",
    "    .start(output_path_2)\n",
    ")\n",
    "\n",
    "# Then in another cell, run this monitoring loop:\n",
    "try:\n",
    "    # Track what records we've already seen\n",
    "    seen_records = set()\n",
    "    \n",
    "    print(\"Starting real-time monitoring\")\n",
    "    \n",
    "    while streaming_query.isActive:\n",
    "        # Read the latest results\n",
    "        results = spark.read.format(\"delta\").load(output_path_2)\n",
    "        \n",
    "        # Create a unique identifier for each record (combination of timestamps)\n",
    "        results = results.withColumn(\n",
    "            \"record_id\", \n",
    "            concat(col(\"pickup_datetime\").cast(\"string\"), col(\"dropoff_datetime\").cast(\"string\"))\n",
    "        )\n",
    "        \n",
    "        # Convert to pandas for easier handling\n",
    "        results_pd = results.toPandas()\n",
    "        \n",
    "        # Check for new records\n",
    "        for _, row in results_pd.iterrows():\n",
    "            record_id = row['record_id']\n",
    "            if record_id not in seen_records:\n",
    "                # New record found!\n",
    "                seen_records.add(record_id)\n",
    "                \n",
    "                # Print the new record details\n",
    "                print(\"\\n\" + \"=\"*50)\n",
    "                print(f\"NEW UPDATE DETECTED at {time.strftime('%H:%M:%S')}\")\n",
    "                print(f\"Window: {row['pickup_datetime']} to {row['dropoff_datetime']}\")\n",
    "                print(f\"Processing delay: {row['delay']} ms\")\n",
    "                print(\"-\"*50)\n",
    "                print(\"Top 10 Routes:\")\n",
    "                \n",
    "                for i in range(1, 11):\n",
    "                    start = row[f'start_cell_id_{i}']\n",
    "                    end = row[f'end_cell_id_{i}']\n",
    "                    if pd.notna(start) and pd.notna(end):\n",
    "                        print(f\"  #{i}: {start} → {end}\")\n",
    "                    else:\n",
    "                        print(f\"  #{i}: No data\")\n",
    "                print(\"=\"*50)\n",
    "        \n",
    "        # Wait 5 seconds before checking again\n",
    "        time.sleep(5)\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nMonitoring stopped.\")\n",
    "    \n",
    "finally:\n",
    "    # Optional: You can stop the query here or let it run\n",
    "    # streaming_query.stop()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46223df4-05ed-4aac-9ba2-4ca998f6ec9b",
   "metadata": {},
   "source": [
    "# Query 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833856ff-bd61-4702-9aa1-f737a5ec35e1",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d4a9b14c-a881-471c-b7ea-11a1e0e54ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+--------------------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "|medallion                       |hack_license                    |pickup_datetime    |dropoff_datetime   |trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|total_amount|\n",
      "+--------------------------------+--------------------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "|9C07428094868EDE6CCC840C0332EE34|9C9DB7B440AACF2D056E19B784B0AA3F|2013-01-01 01:28:00|2013-01-01 01:49:00|1260             |3.77         |-73.980621      |40.747913      |-73.977676       |40.782967       |CRD         |16.5       |0.5      |0.5    |3.4       |0.0         |20.9        |\n",
      "|9DA4949196FD93E20A63C67B1916F38D|3AC0043FD88979C986529DA9D1EB0838|2013-01-01 01:47:00|2013-01-01 01:49:00|120              |0.42         |-73.913353      |40.704407      |-73.919273       |40.707863       |CSH         |3.5        |0.5      |0.5    |0.0       |0.0         |4.5         |\n",
      "|9DDC978C91239BCDB2AFBBB14899B4B6|FE97605751125757F80F34DC0E3F1A3B|2013-01-01 01:34:00|2013-01-01 01:49:00|900              |3.25         |-73.956764      |40.766739      |-73.973717       |40.797436       |CRD         |13.5       |0.5      |0.5    |2.8       |0.0         |17.3        |\n",
      "|A2025A43E119F430D715A9DCEB92DA86|9F7AB9860AFC0EACC5A93679E991700D|2013-01-01 01:44:00|2013-01-01 01:49:00|300              |0.51         |-73.98539       |40.717693      |-73.983871       |40.721523       |CRD         |5.0        |0.5      |0.5    |0.0       |0.0         |6.0         |\n",
      "|A32F4B12D699E84231D390AB0CEB4D52|453431ED907BD27018384E8D478FC67F|2013-01-01 01:42:00|2013-01-01 01:49:00|420              |1.3          |-73.989906      |40.719952      |-73.989937       |40.734241       |CRD         |6.5        |0.5      |0.5    |1.0       |0.0         |8.5         |\n",
      "+--------------------------------+--------------------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RE-RUN QUERY 0 TO RE-INIT THE CLEANED DATASET\n",
    "\n",
    "# For inspecting the data\n",
    "rides_df = (spark.read\n",
    "            .schema(schema)\n",
    "            .option(\"header\", \"true\")\n",
    "            .csv(\"input/cleaned_minified_data\") #path\n",
    "            )\n",
    "\n",
    "rides_df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "67228897-69c1-4989-b623-bcedef77a30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_stream_3 = (spark.readStream\n",
    "                .schema(schema)\n",
    "                .option(\"header\", \"true\")\n",
    "                .csv(\"input/cleaned_minified_data\") #path\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f73df6f1-5e0b-4b0f-8967-dbc610b9f5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "\n",
    "# Reuse the grid conversion UDF\n",
    "lat_long_to_grid_udf = udf(lat_long_to_grid, StringType())\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(\"output/profitable_areas\", exist_ok=True)\n",
    "os.makedirs(\"output/empty_taxis_temp\", exist_ok=True)\n",
    "os.makedirs(\"output/_checkpoint_profitable_areas\", exist_ok=True)\n",
    "os.makedirs(\"output/_checkpoint_empty_taxis\", exist_ok=True)\n",
    "\n",
    "# Init Delta tables\n",
    "# Create empty taxis table schema\n",
    "empty_taxis_schema = StructType([\n",
    "    StructField(\"empty_window\", StructType([\n",
    "        StructField(\"start\", TimestampType()),\n",
    "        StructField(\"end\", TimestampType())\n",
    "    ])),\n",
    "    StructField(\"cell_id\", StringType()),\n",
    "    StructField(\"empty_taxis_count\", LongType())\n",
    "])\n",
    "# Create empty taxis table\n",
    "if not os.path.exists(\"output/empty_taxis_temp/_delta_log\"):\n",
    "    spark.createDataFrame([], empty_taxis_schema) \\\n",
    "         .write.format(\"delta\").save(\"output/empty_taxis_temp\")\n",
    "\n",
    "# Stream + Calculate profit metrics per cell\n",
    "profit_per_cell = (\n",
    "    rides_stream_3\n",
    "    .withColumn(\"pickup_cell\", lat_long_to_grid_udf(col(\"pickup_latitude\"), col(\"pickup_longitude\")))\n",
    "    .withColumn(\"dropoff_cell\", lat_long_to_grid_udf(col(\"dropoff_latitude\"), col(\"dropoff_longitude\")))\n",
    "    .filter(col(\"pickup_cell\").isNotNull() & col(\"dropoff_cell\").isNotNull())\n",
    "    .withColumn(\"trip_profit\", col(\"fare_amount\") + col(\"tip_amount\"))\n",
    "    .withWatermark(\"dropoff_datetime\", \"30 minutes\")\n",
    "    .groupBy(\n",
    "        window(col(\"dropoff_datetime\"), \"15 minutes\").alias(\"profit_window\"),\n",
    "        col(\"pickup_cell\")\n",
    "    )\n",
    "    .agg(\n",
    "        approx_percentile(\"trip_profit\", 0.5).alias(\"median_profit\"),\n",
    "        count(\"*\").alias(\"trips_count\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Calc Empty Taxis\n",
    "def calculate_empty_taxis(batch_df, batch_id):\n",
    "    batch_df = (\n",
    "        batch_df\n",
    "        .withColumn(\"pickup_cell\", lat_long_to_grid_udf(col(\"pickup_latitude\"), col(\"pickup_longitude\")))\n",
    "        .withColumn(\"dropoff_cell\", lat_long_to_grid_udf(col(\"dropoff_latitude\"), col(\"dropoff_longitude\")))\n",
    "        .filter(col(\"dropoff_cell\").isNotNull())\n",
    "    )\n",
    "    \n",
    "    # Create windows for each taxi\n",
    "    window_spec = Window.partitionBy(\"medallion\", \"hack_license\").orderBy(\"dropoff_datetime\")\n",
    "    \n",
    "    # Find next pickup after each dropoff\n",
    "    batch_df = (\n",
    "        batch_df\n",
    "        .withColumn(\"next_pickup\", lead(\"pickup_datetime\").over(window_spec))\n",
    "        .withColumn(\"next_pickup_diff\", \n",
    "            when(col(\"next_pickup\").isNotNull(), \n",
    "                 unix_timestamp(col(\"next_pickup\")) - unix_timestamp(col(\"dropoff_datetime\")))\n",
    "        .otherwise(lit(None)))\n",
    "    )\n",
    "    \n",
    "    # ID empty taxis (empty at any point in the last 15 minutes)\n",
    "    empty_taxis = (\n",
    "        batch_df\n",
    "        .filter(\n",
    "            (col(\"next_pickup\").isNull()) |\n",
    "            ((col(\"next_pickup_diff\") > 900) & (col(\"next_pickup_diff\") <= 1800)) # Empty >= 15 mins <= 30\n",
    "        )\n",
    "        .groupBy(\n",
    "            window(col(\"dropoff_datetime\"), \"15 minutes\").alias(\"empty_window\"),\n",
    "            col(\"dropoff_cell\").alias(\"cell_id\")\n",
    "        )\n",
    "        .agg(count(\"*\").alias(\"empty_taxis_count\"))\n",
    "    )\n",
    "    \n",
    "    # Keep relevant empty_taxi_data (last 30 minutes)\n",
    "    empty_taxis = empty_taxis.withWatermark(\"empty_window\", \"30 minutes\")\n",
    "    \n",
    "    # write\n",
    "    empty_taxis.write.format(\"delta\").mode(\"append\").save(\"output/empty_taxis_temp\")\n",
    "\n",
    "\n",
    "# Start empty taxis stream\n",
    "empty_taxis_stream = (\n",
    "    rides_stream_3\n",
    "    .writeStream\n",
    "    .foreachBatch(calculate_empty_taxis)\n",
    "    .outputMode(\"update\")\n",
    "    .option(\"checkpointLocation\", \"output/_checkpoint_empty_taxis\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "def calculate_profitability(batch_df, batch_id):\n",
    "    try:\n",
    "        empty_taxis = spark.read.format(\"delta\").load(\"output/empty_taxis_temp\")\n",
    "        \n",
    "        profitability = (\n",
    "            batch_df\n",
    "            .join(\n",
    "                empty_taxis,\n",
    "                (col(\"profit_window\") == col(\"empty_window\")) &\n",
    "                (col(\"pickup_cell\") == col(\"cell_id\")),\n",
    "                \"inner\"\n",
    "            )\n",
    "            .withColumn(\"profitability\", \n",
    "                col(\"median_profit\") / greatest(col(\"empty_taxis_count\"), lit(1))\n",
    "            )\n",
    "            .select(\n",
    "                col(\"profit_window.start\").alias(\"analysis_window_start\"),\n",
    "                col(\"profit_window.end\").alias(\"analysis_window_end\"),\n",
    "                col(\"pickup_cell\").alias(\"profitable_cell_id\"),\n",
    "                col(\"empty_taxis_count\").alias(\"empty_taxis_in_cell\"),\n",
    "                col(\"median_profit\").alias(\"median_profit_in_cell\"),\n",
    "                col(\"profitability\").alias(\"profitability_of_cell\"),\n",
    "                col(\"trips_count\").alias(\"trips_count\")\n",
    "            )\n",
    "            .orderBy(col(\"profitability_of_cell\").desc())\n",
    "            .limit(10)\n",
    "        )\n",
    "        \n",
    "        profitability.write.format(\"delta\").mode(\"overwrite\").save(\"output/profitable_areas\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch {batch_id}: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "# Final Output Stream\n",
    "profitability_stream = (\n",
    "    profit_per_cell\n",
    "    .writeStream\n",
    "    .foreachBatch(calculate_profitability)\n",
    "    .outputMode(\"complete\")\n",
    "    .option(\"checkpointLocation\", \"output/_checkpoint_profitable_areas\")\n",
    "    .trigger(processingTime=\"15 minutes\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# Handle stream interruption gracefully\n",
    "try:\n",
    "    profitability_stream.awaitTermination(300)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopping streams gracefully...\")\n",
    "    empty_taxis_stream.stop()\n",
    "    profitability_stream.stop()\n",
    "    time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2c9891f2-cf11-4d81-90cd-60c7b5691f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[analysis_window_start: timestamp, analysis_window_end: timestamp, profitable_cell_id: string, empty_taxis_in_cell: bigint, median_profit_in_cell: double, profitability_of_cell: double, trips_count: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "profitability_df = spark.read.format(\"delta\").load(\"output/profitable_areas\")\n",
    "display(profitability_df.orderBy(col(\"analysis_window_start\").desc()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bf8147ff-b5c6-438d-8490-260df3ccd406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analysis_window_start</th>\n",
       "      <th>analysis_window_end</th>\n",
       "      <th>profitable_cell_id</th>\n",
       "      <th>empty_taxis_in_cell</th>\n",
       "      <th>median_profit_in_cell</th>\n",
       "      <th>profitability_of_cell</th>\n",
       "      <th>trips_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01 00:30:00</td>\n",
       "      <td>2013-01-01 00:45:00</td>\n",
       "      <td>188.126</td>\n",
       "      <td>1</td>\n",
       "      <td>100.4</td>\n",
       "      <td>100.4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-01 00:30:00</td>\n",
       "      <td>2013-01-01 00:45:00</td>\n",
       "      <td>188.126</td>\n",
       "      <td>1</td>\n",
       "      <td>100.4</td>\n",
       "      <td>100.4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-01 01:15:00</td>\n",
       "      <td>2013-01-01 01:30:00</td>\n",
       "      <td>147.161</td>\n",
       "      <td>1</td>\n",
       "      <td>87.5</td>\n",
       "      <td>87.5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-01 01:15:00</td>\n",
       "      <td>2013-01-01 01:30:00</td>\n",
       "      <td>147.161</td>\n",
       "      <td>1</td>\n",
       "      <td>87.5</td>\n",
       "      <td>87.5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-01 01:30:00</td>\n",
       "      <td>2013-01-01 01:45:00</td>\n",
       "      <td>147.162</td>\n",
       "      <td>1</td>\n",
       "      <td>78.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2013-01-01 01:00:00</td>\n",
       "      <td>2013-01-01 01:15:00</td>\n",
       "      <td>146.164</td>\n",
       "      <td>1</td>\n",
       "      <td>77.4</td>\n",
       "      <td>77.4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2013-01-01 01:00:00</td>\n",
       "      <td>2013-01-01 01:15:00</td>\n",
       "      <td>146.164</td>\n",
       "      <td>1</td>\n",
       "      <td>77.4</td>\n",
       "      <td>77.4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2013-01-01 01:45:00</td>\n",
       "      <td>2013-01-01 02:00:00</td>\n",
       "      <td>147.164</td>\n",
       "      <td>1</td>\n",
       "      <td>77.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2013-01-01 01:45:00</td>\n",
       "      <td>2013-01-01 02:00:00</td>\n",
       "      <td>147.164</td>\n",
       "      <td>1</td>\n",
       "      <td>77.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2013-01-01 01:30:00</td>\n",
       "      <td>2013-01-01 01:45:00</td>\n",
       "      <td>147.161</td>\n",
       "      <td>1</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  analysis_window_start analysis_window_end profitable_cell_id  \\\n",
       "0   2013-01-01 00:30:00 2013-01-01 00:45:00            188.126   \n",
       "1   2013-01-01 00:30:00 2013-01-01 00:45:00            188.126   \n",
       "2   2013-01-01 01:15:00 2013-01-01 01:30:00            147.161   \n",
       "3   2013-01-01 01:15:00 2013-01-01 01:30:00            147.161   \n",
       "4   2013-01-01 01:30:00 2013-01-01 01:45:00            147.162   \n",
       "5   2013-01-01 01:00:00 2013-01-01 01:15:00            146.164   \n",
       "6   2013-01-01 01:00:00 2013-01-01 01:15:00            146.164   \n",
       "7   2013-01-01 01:45:00 2013-01-01 02:00:00            147.164   \n",
       "8   2013-01-01 01:45:00 2013-01-01 02:00:00            147.164   \n",
       "9   2013-01-01 01:30:00 2013-01-01 01:45:00            147.161   \n",
       "\n",
       "   empty_taxis_in_cell  median_profit_in_cell  profitability_of_cell  \\\n",
       "0                    1                  100.4                  100.4   \n",
       "1                    1                  100.4                  100.4   \n",
       "2                    1                   87.5                   87.5   \n",
       "3                    1                   87.5                   87.5   \n",
       "4                    1                   78.0                   78.0   \n",
       "5                    1                   77.4                   77.4   \n",
       "6                    1                   77.4                   77.4   \n",
       "7                    1                   77.0                   77.0   \n",
       "8                    1                   77.0                   77.0   \n",
       "9                    1                   75.0                   75.0   \n",
       "\n",
       "   trips_count  \n",
       "0            4  \n",
       "1            4  \n",
       "2            4  \n",
       "3            4  \n",
       "4            4  \n",
       "5            4  \n",
       "6            4  \n",
       "7            4  \n",
       "8            4  \n",
       "9            4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "profitability_pd = profitability_df.orderBy(col(\"profitability_of_cell\").desc()).toPandas()\n",
    "display(profitability_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa12e04-005a-49b2-b510-3cde0b43a479",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b5692be2-2250-4acd-9e2a-072866f932f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime # Stick w/ 1 date library next time (-_-)\n",
    "\n",
    "def transform_to_wide_format(batch_df, batch_id):\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        # Read the profitability data + grab headers\n",
    "        profitability_df = spark.read.format(\"delta\").load(\"output/profitable_areas\")\n",
    "\n",
    "        latest_record = profitability_df.orderBy(col(\"analysis_window_start\").desc()).first()\n",
    "        \n",
    "        if latest_record is None:\n",
    "            return\n",
    "        \n",
    "        # Create a window for ranking\n",
    "        window_spec = Window.orderBy(col(\"profitability_of_cell\").desc())\n",
    "        \n",
    "        # Rank +pivot data\n",
    "        ranked_df = (\n",
    "            profitability_df\n",
    "            .withColumn(\"rank\", rank().over(window_spec))\n",
    "            .filter(col(\"rank\") <= 10)  # Only keep top 10\n",
    "        )\n",
    "        \n",
    "        # Append 10 columns per type + format column names\n",
    "        wide_df = (\n",
    "            ranked_df\n",
    "            .groupBy(\"analysis_window_start\", \"analysis_window_end\")\n",
    "            .pivot(\"rank\", range(1, 11))\n",
    "            .agg(\n",
    "                first(\"profitable_cell_id\").alias(\"profitable_cell_id\"),\n",
    "                first(\"empty_taxis_in_cell\").alias(\"empty_taxis_in_cell\"),\n",
    "                first(\"median_profit_in_cell\").alias(\"median_profit_in_cell\"),\n",
    "                first(\"profitability_of_cell\").alias(\"profitability_of_cell\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "        for i in range(1, 11):\n",
    "            wide_df = (\n",
    "                wide_df\n",
    "                .withColumnRenamed(f\"{i}_profitable_cell_id\", f\"profitable_cell_id_{i}\")\n",
    "                .withColumnRenamed(f\"{i}_empty_taxis_in_cell\", f\"empty_taxis_in_cell_{i}\")\n",
    "                .withColumnRenamed(f\"{i}_median_profit_in_cell\", f\"median_profit_in_cell_{i}\")\n",
    "                .withColumnRenamed(f\"{i}_profitability_of_cell\", f\"profitability_of_cell_{i}\")\n",
    "            )\n",
    "        \n",
    "        # Fill NULLs for ranks that don't exist\n",
    "        for i in range(1, 11):\n",
    "            for col_suffix in [\"profitable_cell_id\", \"empty_taxis_in_cell\", \n",
    "                              \"median_profit_in_cell\", \"profitability_of_cell\"]:\n",
    "                col_name = f\"{col_suffix}_{i}\"\n",
    "                if col_name not in wide_df.columns:\n",
    "                    wide_df = wide_df.withColumn(col_name, lit(None))\n",
    "        \n",
    "        # Calculate processing delay + add delay column\n",
    "        processing_time = datetime.now()\n",
    "        delay_seconds = (processing_time - start_time).total_seconds()\n",
    "        \n",
    "        wide_df = wide_df.withColumn(\"delay\", lit(delay_seconds))\n",
    "\n",
    "        # write\n",
    "        wide_df.write.format(\"delta\").mode(\"overwrite\").save(\"output/profitable_areas_wide_format\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch {batch_id}: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "# Create the output stream\n",
    "wide_format_stream = (\n",
    "    profit_per_cell  # Same profit stream from Query 2 Part 1\n",
    "    .writeStream\n",
    "    .foreachBatch(transform_to_wide_format)\n",
    "    .outputMode(\"update\")\n",
    "    .option(\"checkpointLocation\", \"output/profitable_areas_part2\")\n",
    "    .trigger(processingTime=\"1 minute\")\n",
    "    .start()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5c412585-d5f5-4f53-9206-6846d7dbd98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[analysis_window_start: timestamp, analysis_window_end: timestamp, 1_profitable_cell_id: string, 1_empty_taxis_in_cell: bigint, 1_median_profit_in_cell: double, 1_profitability_of_cell: double, 2_profitable_cell_id: string, 2_empty_taxis_in_cell: bigint, 2_median_profit_in_cell: double, 2_profitability_of_cell: double, 3_profitable_cell_id: string, 3_empty_taxis_in_cell: bigint, 3_median_profit_in_cell: double, 3_profitability_of_cell: double, 4_profitable_cell_id: string, 4_empty_taxis_in_cell: bigint, 4_median_profit_in_cell: double, 4_profitability_of_cell: double, 5_profitable_cell_id: string, 5_empty_taxis_in_cell: bigint, 5_median_profit_in_cell: double, 5_profitability_of_cell: double, 6_profitable_cell_id: string, 6_empty_taxis_in_cell: bigint, 6_median_profit_in_cell: double, 6_profitability_of_cell: double, 7_profitable_cell_id: string, 7_empty_taxis_in_cell: bigint, 7_median_profit_in_cell: double, 7_profitability_of_cell: double, 8_profitable_cell_id: string, 8_empty_taxis_in_cell: bigint, 8_median_profit_in_cell: double, 8_profitability_of_cell: double, 9_profitable_cell_id: string, 9_empty_taxis_in_cell: bigint, 9_median_profit_in_cell: double, 9_profitability_of_cell: double, 10_profitable_cell_id: string, 10_empty_taxis_in_cell: bigint, 10_median_profit_in_cell: double, 10_profitability_of_cell: double, delay: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "part2_df = spark.read.format(\"delta\").load(\"output/profitable_areas_part2\")\n",
    "display(part2_df.orderBy(col(\"analysis_window_start\").desc()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ab252748-c0ee-43f7-96cc-a319ac842085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analysis_window_start</th>\n",
       "      <th>analysis_window_end</th>\n",
       "      <th>1_profitable_cell_id</th>\n",
       "      <th>1_empty_taxis_in_cell</th>\n",
       "      <th>1_median_profit_in_cell</th>\n",
       "      <th>1_profitability_of_cell</th>\n",
       "      <th>2_profitable_cell_id</th>\n",
       "      <th>2_empty_taxis_in_cell</th>\n",
       "      <th>2_median_profit_in_cell</th>\n",
       "      <th>2_profitability_of_cell</th>\n",
       "      <th>3_profitable_cell_id</th>\n",
       "      <th>3_empty_taxis_in_cell</th>\n",
       "      <th>3_median_profit_in_cell</th>\n",
       "      <th>3_profitability_of_cell</th>\n",
       "      <th>4_profitable_cell_id</th>\n",
       "      <th>4_empty_taxis_in_cell</th>\n",
       "      <th>4_median_profit_in_cell</th>\n",
       "      <th>4_profitability_of_cell</th>\n",
       "      <th>5_profitable_cell_id</th>\n",
       "      <th>5_empty_taxis_in_cell</th>\n",
       "      <th>5_median_profit_in_cell</th>\n",
       "      <th>5_profitability_of_cell</th>\n",
       "      <th>6_profitable_cell_id</th>\n",
       "      <th>6_empty_taxis_in_cell</th>\n",
       "      <th>6_median_profit_in_cell</th>\n",
       "      <th>6_profitability_of_cell</th>\n",
       "      <th>7_profitable_cell_id</th>\n",
       "      <th>7_empty_taxis_in_cell</th>\n",
       "      <th>7_median_profit_in_cell</th>\n",
       "      <th>7_profitability_of_cell</th>\n",
       "      <th>8_profitable_cell_id</th>\n",
       "      <th>8_empty_taxis_in_cell</th>\n",
       "      <th>8_median_profit_in_cell</th>\n",
       "      <th>8_profitability_of_cell</th>\n",
       "      <th>9_profitable_cell_id</th>\n",
       "      <th>9_empty_taxis_in_cell</th>\n",
       "      <th>9_median_profit_in_cell</th>\n",
       "      <th>9_profitability_of_cell</th>\n",
       "      <th>10_profitable_cell_id</th>\n",
       "      <th>10_empty_taxis_in_cell</th>\n",
       "      <th>10_median_profit_in_cell</th>\n",
       "      <th>10_profitability_of_cell</th>\n",
       "      <th>delay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01 01:45:00</td>\n",
       "      <td>2013-01-01 02:00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>147.164</td>\n",
       "      <td>1.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.96011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-01 01:30:00</td>\n",
       "      <td>2013-01-01 01:45:00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>147.162</td>\n",
       "      <td>1.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>147.161</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.96011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-01 01:15:00</td>\n",
       "      <td>2013-01-01 01:30:00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>147.161</td>\n",
       "      <td>1.0</td>\n",
       "      <td>87.5</td>\n",
       "      <td>87.5</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.96011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-01 01:00:00</td>\n",
       "      <td>2013-01-01 01:15:00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>146.164</td>\n",
       "      <td>1.0</td>\n",
       "      <td>77.4</td>\n",
       "      <td>77.4</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.96011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-01 00:30:00</td>\n",
       "      <td>2013-01-01 00:45:00</td>\n",
       "      <td>188.126</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.4</td>\n",
       "      <td>100.4</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.96011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Read wide_data\n",
    "part2_delta_df = spark.read.format(\"delta\").load(\"output/profitable_areas_part2\")\n",
    "pdf = part2_delta_df.orderBy(col(\"analysis_window_start\").desc()).limit(20).toPandas()\n",
    "\n",
    "# Display\n",
    "display(HTML(pdf.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7212fa-05e8-4ffd-b322-707566df79ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
